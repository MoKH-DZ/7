import requests
from bs4 import BeautifulSoup
import sqlite3
from telegram import Bot, InputMediaPhoto
from telegram.error import TelegramError
import time
import schedule
import random
import logging
from datetime import datetime
import json
import os

# ===== ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ³Ø¬ÙŠÙ„ =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ouedkniss_monitor.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('OuedknissMonitor')

# ===== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ† =====
class Config:
    TELEGRAM_TOKEN = "8049690165:AAEYWPJzpggMfwEXykjG-ybYFc8poRtY9_0"
    CHAT_ID = "5686817749"
    CHECK_INTERVAL = 5  # Ø¯Ù‚Ø§Ø¦Ù‚ Ø¨ÙŠÙ† Ø§Ù„ÙØ­ÙˆØµØ§Øª
    REQUEST_TIMEOUT = 25  # Ø«Ø§Ù†ÙŠØ© Ù„ÙƒÙ„ Ø·Ù„Ø¨
    MAX_RETRIES = 3  # Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø¥Ø¹Ø§Ø¯Ø© Ù„ÙƒÙ„ Ø·Ù„Ø¨ ÙØ§Ø´Ù„

    BASE_URL = "https://www.ouedkniss.com/automobiles_vehicules/1?keywords="

    KEYWORDS = [
        "Transporter", "Multivan", "Transporteur", "Caravelle", "Kombi",
        "Ø·Ø±ÙˆØ³Ø¨ÙˆØ±ØªØ§Ø±", "Ù…ÙŠÙ„ØªÙŠÙØ§Ù†", "T5", "T6", "T6.1", "Ø·Ø±Ø§Ù†Ø³Ø¨ÙˆØ±ØªØ§Ùˆ", "golf"
    ]

    WILAYAS = []  # Ù…Ø«Ø§Ù„: ["16", "31"] Ù„Ù„Ø¬Ø²Ø§Ø¦Ø± Ø§Ù„Ø¹Ø§ØµÙ…Ø© ÙˆÙˆÙ‡Ø±Ø§Ù†

    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36',
        'Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1'
    ]

    PROXY_LIST = []  # Ù‚Ù… Ø¨Ù…Ù„Ø¡ Ù‡Ø°Ù‡ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ³ØªØ®Ø¯Ù… Ø¨Ø±ÙˆÙƒØ³ÙŠ

    @classmethod
    def get_random_proxy(cls):
        return random.choice(cls.PROXY_LIST) if cls.PROXY_LIST else None

# ===== Ø¥Ø¯Ø§Ø±Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª =====
class DatabaseManager:
    def __init__(self):
        self.db_path = 'ouedkniss_listings.db'
        self._init_db()

    def _init_db(self):
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS listings (
                id TEXT PRIMARY KEY,
                title TEXT,
                price TEXT,
                url TEXT,
                keyword TEXT,
                wilaya TEXT,
                image_url TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                notified BOOLEAN DEFAULT 0
            )
        ''')

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø§Ø±Ø³
        self.cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_listing_keyword 
            ON listings (keyword)
        ''')

        self.conn.commit()

    def listing_exists(self, listing_id):
        self.cursor.execute('''
            SELECT 1 FROM listings WHERE id = ?
        ''', (listing_id,))
        return self.cursor.fetchone() is not None

    def add_listing(self, listing_data):
        try:
            self.cursor.execute('''
                INSERT INTO listings 
                (id, title, price, url, keyword, wilaya, image_url, notified)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                listing_data['id'],
                listing_data['title'],
                listing_data['price'],
                listing_data['url'],
                listing_data['keyword'],
                listing_data['wilaya'],
                listing_data['image_url'],
                1 if listing_data.get('notified', False) else 0
            ))
            self.conn.commit()
            return True
        except sqlite3.IntegrityError:
            return False

    def get_unnotified_listings(self):
        self.cursor.execute('''
            SELECT * FROM listings WHERE notified = 0 LIMIT 10
        ''')
        columns = [col[0] for col in self.cursor.description]
        return [dict(zip(columns, row)) for row in self.cursor.fetchall()]

    def mark_as_notified(self, listing_id):
        self.cursor.execute('''
            UPDATE listings SET notified = 1 WHERE id = ?
        ''', (listing_id,))
        self.conn.commit()

    def close(self):
        self.conn.close()

# ===== Ù…Ø¯ÙŠØ± Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª =====
class AlertManager:
    def __init__(self):
        self.bot = Bot(token=Config.TELEGRAM_TOKEN)
        self.last_notification_time = 0
        self.notification_cooldown = 2  # Ø«Ø§Ù†ÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ø¥Ø´Ø¹Ø§Ø±Ø§Øª

    def send_alert(self, listing):
        current_time = time.time()
        time_since_last = current_time - self.last_notification_time

        if time_since_last < self.notification_cooldown:
            time.sleep(self.notification_cooldown - time_since_last)

        try:
            message = self._format_message(listing)

            if listing.get('image_url'):
                self._send_photo_alert(listing['image_url'], message)
            else:
                self._send_text_alert(message)

            self.last_notification_time = time.time()
            return True

        except Exception as e:
            logger.error(f"ÙØ´Ù„ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡: {str(e)}")
            return False

    def _format_message(self, listing):
        return (
            f"ğŸš— **Ø¥Ø¹Ù„Ø§Ù† Ø¬Ø¯ÙŠØ¯ ({listing['keyword']})**\n\n"
            f"ğŸ“ **Ø§Ù„ÙˆÙ„Ø§ÙŠØ©:** {listing.get('wilaya', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}\n"
            f"ğŸ”¹ **Ø§Ù„Ø¹Ù†ÙˆØ§Ù†:** {listing['title']}\n"
            f"ğŸ’° **Ø§Ù„Ø³Ø¹Ø±:** {listing['price']}\n\n"
            f"ğŸ”— [Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù† Ø¹Ù„Ù‰ Ouedkniss]({listing['url']})\n"
            f"ğŸ•’ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        )

    def _send_photo_alert(self, image_url, message):
        try:
            self.bot.send_photo(
                chat_id=Config.CHAT_ID,
                photo=image_url,
                caption=message,
                parse_mode='Markdown',
                timeout=Config.REQUEST_TIMEOUT
            )
        except Exception as e:
            logger.warning(f"ÙØ´Ù„ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØµÙˆØ±Ø©: {str(e)}ØŒ Ø¬Ø§Ø±Ù Ø¥Ø±Ø³Ø§Ù„ Ù†Øµ ÙÙ‚Ø·...")
            self._send_text_alert(message)

    def _send_text_alert(self, message):
        self.bot.send_message(
            chat_id=Config.CHAT_ID,
            text=message,
            parse_mode='Markdown',
            disable_web_page_preview=False,
            timeout=Config.REQUEST_TIMEOUT
        )

# ===== Ù…Ø¯ÙŠØ± Ø§Ù„Ø²Ø­Ù =====
class CrawlerManager:
    def __init__(self):
        self.db = DatabaseManager()
        self.alert = AlertManager()
        self.session = requests.Session()
        self.session.headers.update({
            'Accept-Language': 'ar,fr-FR;q=0.9,fr;q=0.8,en-US;q=0.7,en;q=0.6',
            'Referer': 'https://www.ouedkniss.com/'
        })

    def scrape_keyword(self, keyword):
        url = self._build_search_url(keyword)

        for attempt in range(Config.MAX_RETRIES):
            try:
                response = self._make_request(url)
                return self._parse_response(response, keyword)
            except Exception as e:
                logger.warning(f"Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1} ÙØ´Ù„Øª Ù„Ù„ÙƒÙ„Ù…Ø© '{keyword}': {str(e)}")
                if attempt == Config.MAX_RETRIES - 1:
                    logger.error(f"ÙØ´Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ù„Ù„ÙƒÙ„Ù…Ø© '{keyword}'")
                    return []
                time.sleep(random.uniform(2, 5))

    def _build_search_url(self, keyword):
        url = Config.BASE_URL + requests.utils.quote(keyword)
        if Config.WILAYAS:
            url += f"&w={','.join(Config.WILAYAS)}"
        return url

    def _make_request(self, url):
        proxy = Config.get_random_proxy()
        headers = {'User-Agent': random.choice(Config.USER_AGENTS)}

        response = self.session.get(
            url,
            headers=headers,
            timeout=Config.REQUEST_TIMEOUT,
            proxies={'http': proxy, 'https': proxy} if proxy else None
        )
        response.raise_for_status()
        return response

    def _parse_response(self, response, keyword):
        soup = BeautifulSoup(response.text, 'html.parser')
        listings = soup.find_all('li', attrs={"data-id": True})
        new_listings = []

        for listing in listings:
            listing_data = self._extract_listing_data(listing, keyword)
            if listing_data and not self.db.listing_exists(listing_data['id']):
                if self.db.add_listing(listing_data):
                    new_listings.append(listing_data)

        return new_listings

    def _extract_listing_data(self, listing, keyword):
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆÙ‚Øª Ø§Ù„Ù†Ø´Ø±
        time_element = listing.find('span', class_='annonce_date')
        if not time_element:
            return None

        time_text = time_element.get_text(strip=True).lower()
        if not any(x in time_text for x in ['minute', 'minutes', 'Ø§Ù„Ø¢Ù†', 'heure', 'hour', 'Ø³Ø§Ø¹Ø©']):
            return None

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†
        listing_id = listing.get("data-id")
        title = listing.find('h2').get_text(strip=True) if listing.find('h2') else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
        price = listing.find('span', class_='annonce_prix').get_text(strip=True) if listing.find('span', class_='annonce_prix') else "ØºÙŠØ± Ù…Ø°ÙƒÙˆØ±"
        link = listing.find('a', href=True)
        listing_url = "https://www.ouedkniss.com" + link['href'] if link else None

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±Ø© (ÙŠØ¯Ø¹Ù… Ø§Ù„Ø³Ù…Ø© data-src)
        image = listing.find('img')
        image_url = None
        if image:
            image_url = image.get('src') or image.get('data-src')
            if image_url and image_url.startswith('//'):
                image_url = 'https:' + image_url

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆÙ„Ø§ÙŠØ©
        wilaya = None
        location = listing.find('span', class_='annonce_region')
        if location:
            wilaya = location.get_text(strip=True).split(' ')[0]

        return {
            'id': listing_id,
            'title': title,
            'price': price,
            'url': listing_url,
            'keyword': keyword,
            'wilaya': wilaya,
            'image_url': image_url
        }

# ===== Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© =====
def main():
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø¨ÙˆØª Ù…Ø±Ø§Ù‚Ø¨Ø© Ouedkniss")

    # Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„
    alert = AlertManager()
    try:
        alert.send_alert({
            'keyword': 'Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©',
            'title': 'Ø¨Ø¯Ø£ ØªØ´ØºÙŠÙ„ Ø¨ÙˆØª Ù…Ø±Ø§Ù‚Ø¨Ø© Ouedkniss',
            'price': '',
            'url': '',
            'wilaya': '',
            'image_url': None
        })
    except Exception as e:
        logger.error(f"ÙØ´Ù„ Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¨Ø¯Ø¡: {str(e)}")

    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø¯ÙŠØ±ÙŠÙ†
    crawler = CrawlerManager()

    # Ø¬Ø¯ÙˆÙ„Ø© Ø§Ù„Ù…Ù‡Ø§Ù…
    def monitoring_job():
        logger.info("Ø¨Ø¯Ø¡ Ø¯ÙˆØ±Ø© Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©")
        start_time = time.time()

        for keyword in Config.KEYWORDS:
            try:
                logger.info(f"Ø¬Ø§Ø±Ù ÙØ­Øµ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©: {keyword}")
                new_listings = crawler.scrape_keyword(keyword)

                for listing in new_listings:
                    if alert.send_alert(listing):
                        crawler.db.mark_as_notified(listing['id'])
                    time.sleep(random.uniform(1, 3))  # ØªØ£Ø®ÙŠØ± Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¨ÙŠÙ† Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª

                # ØªØ£Ø®ÙŠØ± Ø¨ÙŠÙ† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
                time.sleep(random.uniform(2, 5))

            except Exception as e:
                logger.error(f"Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒÙ„Ù…Ø© '{keyword}': {str(e)}")

        elapsed = time.time() - start_time
        logger.info(f"Ø§ÙƒØªÙ…Ù„Øª Ø¯ÙˆØ±Ø© Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙŠ {elapsed:.2f} Ø«Ø§Ù†ÙŠØ©")

    # Ø§Ù„Ø¬Ø¯ÙˆÙ„Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
    schedule.every(Config.CHECK_INTERVAL).minutes.do(monitoring_job)

    # Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙÙˆØ±ÙŠ Ù„Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰
    monitoring_job()

    # Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    try:
        while True:
            schedule.run_pending()
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("ØªÙ„Ù‚ÙŠ Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¥ÙŠÙ‚Ø§ÙØŒ Ø¬Ø§Ø±Ù Ø§Ù„Ø¥ØºÙ„Ø§Ù‚...")
    finally:
        crawler.db.close()
        logger.info("ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨ÙˆØª Ø¨Ù†Ø¬Ø§Ø­")

if __name__ == "__main__":
    main()